{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3fe801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd14228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b95706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ba1b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eda60a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be29d1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45a1d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {idx:name for idx, name in enumerate(news.target_names)}\n",
    "news_df[\"Target\"] = news_df[\"Target\"].replace(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e80a5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Matthew B Lawson Subject Which high perfo...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From Matthew B Lawson Subject Which high perfo...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_cleansing(df):\n",
    "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
    "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
    "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
    "    cleaning_result = ' '.join(delete_non_word.split())\n",
    "    return cleaning_result\n",
    "\n",
    "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a940d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = 'looking looks looked'\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0307228f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711ed6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f99e5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]\n",
    "algorithms = [MultinomialNB(), LogisticRegression()]\n",
    "\n",
    "pipelines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef97bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)):\n",
    "    pipelines.append(make_pipeline(*case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "928fe345",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_params = [(1,1),(1,3)]\n",
    "stopword_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "max_df_params = np.linspace(0.4, 0.6, num=6)\n",
    "min_df_params = np.linspace(0.0, 0.0, num=1)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params, \"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopword_params}\n",
    "vectorizer_names = [\"countvectorizer\",\"tfidfvectorizer\",\"stemmedcountvectorizer\",\"stemmedtfidfvectorizer\"]\n",
    "vectorizer_params_dict = {}\n",
    "\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name] = {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a24ed67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_names = [\"multinomialnb\", \"logisticregression\"]\n",
    "\n",
    "algorithm_params_dict = {}\n",
    "alpha_params = np.linspace(1.0, 1.0, num=1)\n",
    "for i in range(1):\n",
    "    algorithm_params_dict[algorithm_names[i]] = {\n",
    "        algorithm_names[i]+ \"__alpha\" : alpha_params\n",
    "    }\n",
    "c_params = [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "\n",
    "algorithm_params_dict[algorithm_names[1]] = [{\n",
    "    \"logisticregression__multi_class\" : [\"multinomial\"],\n",
    "    \"logisticregression__solver\" : [\"saga\"],\n",
    "    \"logisticregression__penalty\" : [\"l1\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    },{ \n",
    "    \"logisticregression__multi_class\" : [\"ovr\"],\n",
    "    \"logisticregression__solver\" : ['liblinear'],\n",
    "    \"logisticregression__penalty\" : [\"l2\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    }\n",
    "    ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe5c5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params= []\n",
    "for case in list(itertools.product(vectorizer_names, algorithm_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithm_params_dict[case[1]]  \n",
    "    \n",
    "    if isinstance(algo_params, dict):\n",
    "        vect_params.update(algo_params)\n",
    "        pipeline_params.append(vect_params)\n",
    "    else:\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipeline_params.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e8b243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_data = news_df.loc[:, 'News'].tolist()\n",
    "y_data = news_df['Target'].tolist()\n",
    "y = LabelEncoder().fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9759fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
      "                                       ('multinomialnb', MultinomialNB())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'countvectorizer__lowercase': [True, False],\n",
      "                         'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
      "                         'countvectorizer__min_df': array([0.]),\n",
      "                         'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'countvectorizer__stop_words': ['english'],\n",
      "                         'multinomialnb__alpha': array([1.])},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
      "                                       ('logisticregression',\n",
      "                                        LogisticRegression())]),\n",
      "             n_jobs=-1,\n",
      "             param_grid=[{'countvectorizer__lowercase': [True, False],\n",
      "                          'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
      "                          'countvectorizer__min_df': array([0.]),\n",
      "                          'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                          'countvecto...\n",
      "                          'countvectorizer__min_df': array([0.]),\n",
      "                          'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                          'countvectorizer__stop_words': ['english'],\n",
      "                          'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0,\n",
      "                                                    20.0, 100.0],\n",
      "                          'logisticregression__multi_class': ['ovr'],\n",
      "                          'logisticregression__penalty': ['l2'],\n",
      "                          'logisticregression__solver': ['liblinear']}],\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n"
     ]
    }
   ],
   "source": [
    "# 컴퓨터 성능에 따라 실행하면 컴퓨터가 멍출 가능성이 있음\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "scoring = ['accuracy']\n",
    "estimator_results = []\n",
    "for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):\n",
    "    n_jobs = -1\n",
    "    gs_estimator = GridSearchCV(\n",
    "            refit=\"accuracy\", estimator=estimator,param_grid=params,\n",
    "            scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)\n",
    "    print(gs_estimator)\n",
    "    \n",
    "    gs_estimator.fit(X_data, y)\n",
    "    estimator_results.append(gs_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
